{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Análise em Dados do E-commerce da Olist\n",
        "author: Juan Victor Carballo Blanco\n",
        "subtitle: Desafio UFRJ Analyica\n",
        "date: 03/12/2023\n",
        "date-format: DD/MM/YYYY\n",
        "format:\n",
        "  pdf:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    colorlinks: true\n",
        "output:\n",
        "  pdf_document:\n",
        "    df_print: kable\n",
        "    toc: true\n",
        "    toc_depth: 2\n",
        "toc-title: \"Sumário\"\n",
        "---"
      ],
      "id": "809ac04d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\pagebreak\n",
        "\n",
        "# Introdução\n",
        "\n",
        "O trabalho em questão consiste em uma análise do conjunto de dados de e-commerce da [Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce) disponibilizado pelo kaggle. As informações foram criadas entre os anos de 2016 e 2018 consistindo de 8 arquivos de formato \"csv\" que possuem os nomes:\n",
        "\n",
        "- olist_customers_dataset\n",
        "- olist_geolocation_dataset\n",
        "- olist_order_items_dataset\n",
        "- olist_order_payments_dataset\n",
        "- olist_order_reviews_dataset\n",
        "- olist_orders_dataset\n",
        "- olist_orders_dataset\n",
        "- olist_sellers_dataset\n",
        "\n",
        "Todos esses datasets estão interligados através de chaves primárias ou estrangeiros de acordo com a imagem abaixo:\n",
        "\n",
        "![Relacionamento do Conjunto de Dados](DataSchema.png)\n",
        "\n",
        "O objetivo desse projeto é entender quem são os clientes de Olist e por fim realizar um trabalho de segmentação de clientes.\n"
      ],
      "id": "9af5c8b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "import pandas as pd\n",
        "from warnings import filterwarnings\n",
        "\n",
        "filterwarnings('ignore')\n",
        "\n",
        "def import_data():\n",
        "    #  Importando os dados\n",
        "    dataframes_dict = {\n",
        "        \"Customers\": pd.read_csv(\"../Dados/olist_customers_dataset.csv\"),\n",
        "        \"Geolocation\": pd.read_csv(\"../Dados/olist_geolocation_dataset.csv\"),\n",
        "        \"Order_Items\": pd.read_csv(\"../Dados/olist_order_items_dataset.csv\"),\n",
        "        \"Order_Payments\": pd.read_csv(\"../Dados/olist_order_payments_dataset.csv\"),\n",
        "        \"Order_Reviews\": pd.read_csv(\"../Dados/olist_order_reviews_dataset.csv\"),\n",
        "        \"Orders\": pd.read_csv(\"../Dados/olist_orders_dataset.csv\"),\n",
        "        \"Products\": pd.read_csv(\"../Dados/olist_products_dataset.csv\"),\n",
        "        \"Sellers\": pd.read_csv(\"../Dados/olist_sellers_dataset.csv\"),\n",
        "    }\n",
        "\n",
        "    #  Fazendo o tratamento do conjunto de dados dos Consumidores\n",
        "    dataframes_dict[\"Customers\"] = dataframes_dict[\"Customers\"].assign(\n",
        "        customer_state=dataframes_dict[\"Customers\"].customer_state.astype(\"category\")\n",
        "    )\n",
        "\n",
        "    #  Fazendo o tratamento do conjunto de dados dos Pedidos de Clientes\n",
        "    dataframes_dict[\"Order_Items\"] = dataframes_dict[\"Order_Items\"].assign(\n",
        "        shipping_limit_date=pd.to_datetime(\n",
        "            dataframes_dict[\"Order_Items\"][\"shipping_limit_date\"],\n",
        "            format=\"%Y-%m-%d %H:%M:%S\",\n",
        "        ),\n",
        "        total_cost=dataframes_dict[\"Order_Items\"].price\n",
        "        + dataframes_dict[\"Order_Items\"].freight_value,\n",
        "    )\n",
        "\n",
        "    #  Fazendo o tratamento do conjunto de dados dos Tipo de Pagamentos dos Pedidos\n",
        "    dataframes_dict[\"Order_Payments\"] = dataframes_dict[\"Order_Payments\"].assign(\n",
        "        payment_type=dataframes_dict[\"Order_Payments\"].payment_type.astype(\"category\")\n",
        "    )\n",
        "\n",
        "    #  Fazendo o tratamento do conjunto de dados dos Comentários dos Pedidos\n",
        "    dataframes_dict[\"Order_Reviews\"] = dataframes_dict[\"Order_Reviews\"].assign(\n",
        "        review_creation_date=pd.to_datetime(\n",
        "            dataframes_dict[\"Order_Reviews\"][\"review_creation_date\"],\n",
        "            format=\"%Y-%m-%d %H:%M:%S\",\n",
        "        ),\n",
        "        review_answer_timestamp=pd.to_datetime(\n",
        "            dataframes_dict[\"Order_Reviews\"][\"review_answer_timestamp\"],\n",
        "            format=\"%Y-%m-%d %H:%M:%S\",\n",
        "        ),\n",
        "        reviews_reponse_time=lambda x: x.review_answer_timestamp\n",
        "        - x.review_creation_date,\n",
        "    )\n",
        "\n",
        "    #  Fazendo o tratamento do conjunto de dados dos Pedidos\n",
        "    dataframes_dict[\"Orders\"] = (\n",
        "        dataframes_dict[\"Orders\"]\n",
        "        .assign(\n",
        "            order_purchase_timestamp=pd.to_datetime(\n",
        "                dataframes_dict[\"Orders\"][\"order_purchase_timestamp\"],\n",
        "                format=\"%Y-%m-%d %H:%M:%S\",\n",
        "            ),\n",
        "            order_approved_at=pd.to_datetime(\n",
        "                dataframes_dict[\"Orders\"][\"order_approved_at\"],\n",
        "                format=\"%Y-%m-%d %H:%M:%S\",\n",
        "            ),\n",
        "            order_delivered_carrier_date=pd.to_datetime(\n",
        "                dataframes_dict[\"Orders\"][\"order_delivered_carrier_date\"],\n",
        "                format=\"%Y-%m-%d %H:%M:%S\",\n",
        "            ),\n",
        "            order_delivered_customer_date=pd.to_datetime(\n",
        "                dataframes_dict[\"Orders\"][\"order_delivered_customer_date\"],\n",
        "                format=\"%Y-%m-%d %H:%M:%S\",\n",
        "            ),\n",
        "            order_estimated_delivery_date=pd.to_datetime(\n",
        "                dataframes_dict[\"Orders\"][\"order_estimated_delivery_date\"],\n",
        "                format=\"%Y-%m-%d %H:%M:%S\",\n",
        "            ),\n",
        "            order_status=dataframes_dict[\"Orders\"].order_status.astype(\"category\"),\n",
        "            order_purchase_to_approved_time=lambda x: x.order_approved_at\n",
        "            - x.order_purchase_timestamp,\n",
        "            order_observed_delivery_time=lambda x: x.order_delivered_customer_date\n",
        "            - x.order_purchase_timestamp,\n",
        "        )[\n",
        "            [\n",
        "                \"order_id\",\n",
        "                \"customer_id\",\n",
        "                \"order_status\",\n",
        "                \"order_purchase_timestamp\",\n",
        "                \"order_approved_at\",\n",
        "                \"order_purchase_to_approved_time\",\n",
        "                \"order_delivered_carrier_date\",\n",
        "                \"order_delivered_customer_date\",\n",
        "                \"order_observed_delivery_time\",\n",
        "                \"order_estimated_delivery_date\",\n",
        "            ]\n",
        "        ]\n",
        "        .query(\n",
        "            \"not(order_approved_at.isnull() and order_status != 'canceled') and not(order_delivered_carrier_date.isnull() and order_status in ('approved', 'delivered'))\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    #  Fazendo o tratamento do conjunto de dados dos Vendedores\n",
        "    dataframes_dict[\"Sellers\"] = dataframes_dict[\"Sellers\"].assign(\n",
        "        seller_state=dataframes_dict[\"Sellers\"].seller_state.astype(\"category\")\n",
        "    )\n",
        "\n",
        "    return dataframes_dict\n"
      ],
      "id": "993d8f74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise Exploratória de Dados\n",
        "\n",
        "## Compradores e Vendedores\n",
        "\n",
        "Analisando a distribuição espacial dos consumidores, podemos verificar que a demanda da Olist se concentra no sudeste, pricipalmente no estado de São Paulo. Isso provavelmente se deve a maior concentração populacional e economica nessa região que, desde a vinda da Familia Real em 1808, foi se consolidando como a região com maior progresso socio-economico do país. O mapa abaixo mostra essa concentração.\n"
      ],
      "id": "9cc691bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: \"Distribuição de Clientes ao longo do Brasil\"\n",
        "\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn.objects as so\n",
        "\n",
        "\n",
        "data_dict = import_data()\n",
        "\n",
        "def plot_brazilian_maps(state_column_name: str, data: pd.DataFrame, brazil_map: gpd.GeoDataFrame = None, return_br_gpd: bool = False):\n",
        "    \n",
        "    if brazil_map is None:\n",
        "        brazil_map = gpd.read_file(\n",
        "            filename= \"https://geoftp.ibge.gov.br/cartas_e_mapas/bases_cartograficas_continuas/bcim/versao2016/geopackage/bcim_2016_21_11_2018.gpkg\",\n",
        "            layer = 'lim_unidade_federacao_a'\n",
        "        )\n",
        "    data = data[state_column_name].value_counts().reset_index(name='count')\n",
        "    \n",
        "    df = pd.merge(\n",
        "        left=brazil_map,\n",
        "        right=data,\n",
        "        left_on='sigla',\n",
        "        right_on='index',\n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    if return_br_gpd:\n",
        "        return (\n",
        "            df.plot(column='count', cmap='OrRd', figsize=(20, 10), legend=True),\n",
        "            brazil_map\n",
        "        )\n",
        "    else:\n",
        "        return df.plot(column='count', cmap='OrRd', figsize=(20, 10), legend=True)\n",
        "\n",
        "plot_brazilian_maps(\n",
        "    state_column_name='customer_state',\n",
        "    data=data_dict['Customers']\n",
        ")\n",
        "\n",
        "plt.show()"
      ],
      "id": "d1292098",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\pagebreak\n",
        "\n",
        "Quando analisamos o aspecto da oferta, percebemos que as desigualdades regionais se amplificam, principalmente em São Paulo. Nesse aspecto, podemos ressaltar aspectos de escala principalmente no setor indústrial e logístico. No livro Economia Internacional de Paul Krugman no capítulo 7 é destacado como a produção tende a se concentrar em polos devido a ganhos relacionados a escala. A intuição por trás disso é que custos fixos industriais e logisticos, por exemplo, serão o mesmo valor independentemente se instalado em áreas grande dinamismo economico ou de baixo. Dessa forma, é muito mais eficiente e menos custoso a produção ser concentrado em um só lugar do que descentralizada em vários lugares diferentes. No caso em questão, uma empresa de e-commerce como a Olist consegue reduzir seus custos concentrando a produção em um lugar, já que se beneficiaria desse aumento de escala tanto vinda de si própria como de empresas na região.\n"
      ],
      "id": "c45eea88"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: \"Distribuição de Varejistas ao longo do Brasil\"\n",
        "\n",
        "\n",
        "plot_brazilian_maps(\n",
        "    state_column_name='seller_state',\n",
        "    data=data_dict['Sellers']\n",
        ")\n",
        "plt.show()"
      ],
      "id": "a4fda600",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O gráfico abaixo nos mostra novamente essa diferença entre consumidores e vendedores onde temos tanto na oferta quanto na demanda a proporção demandada/ofertada em relação ao volume total. Alguns poucos Estados (São Paulo, Paraná e Santa Catarina) possuem mais vendedores que consumidores. Podemos pensar que, nesse caso, esses estados estivessem \"exportando\" produtos enquanto os demais estivessem \"importando\".\n"
      ],
      "id": "e7c96217"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "\n",
        "p = (\n",
        "    so.Plot(\n",
        "        data = pd.concat([\n",
        "            (\n",
        "                data_dict['Sellers']\n",
        "                .seller_state\n",
        "                .value_counts(normalize=True)\n",
        "                .reset_index()\n",
        "                .assign(Tipo = 'Vendedores')\n",
        "                .rename(columns={'index': 'Estado', 'seller_state': 'Porcentagem'})\n",
        "            ),\n",
        "            (\n",
        "                data_dict['Customers']\n",
        "                .customer_state\n",
        "                .value_counts(normalize=True)\n",
        "                .reset_index()\n",
        "                .assign(Tipo = 'Consumidores')\n",
        "                .rename(columns={'index': 'Estado', 'customer_state' : 'Porcentagem'})\n",
        "            )\n",
        "        ]).reset_index(drop=True),\n",
        "        x='Estado',\n",
        "        y='Porcentagem',\n",
        "        color='Tipo'\n",
        "        \n",
        "    )\n",
        "    .layout(size=(10, 8))\n",
        "    .add(so.Bar(), so.Dodge())\n",
        ")\n",
        "\n",
        "p.show()"
      ],
      "id": "4aeca050",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Produto\n",
        "\n",
        "Os produtos mais vendidos na Olist são produtos relacionados a vida de uma jovem familia brasileira. Infelizmente, não temos acesso a dados demográficos, porém, em vista dos 10 produtos mais vendidos, eu diria que o cliente médio são adultos de mais de 30 anos, pois grande parte desses produtos são relacionados a itens de casa.\n"
      ],
      "id": "b1e7013f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "join_order_items_and_products = pd.merge(\n",
        "    right=data_dict['Order_Items'],\n",
        "    left=data_dict['Products'],\n",
        "    how='left',\n",
        "    on='product_id'\n",
        ")[[\n",
        "    'product_category_name',\n",
        "    'price',\n",
        "    'freight_value',\n",
        "    'total_cost'\n",
        "]]\n",
        "\n",
        "p = (\n",
        "    so.Plot(\n",
        "        data=(join_order_items_and_products.product_category_name.value_counts(normalize=True) * 100).head(10).reset_index(),\n",
        "        y = 'index',\n",
        "        x = 'product_category_name'\n",
        "    )\n",
        "    .add(so.Bar(), so.Dodge())\n",
        "    .layout(size=(15, 8))\n",
        "    .label(x='Porcentagem do Volume Total', y='Produtos')\n",
        ")\n",
        "\n",
        "p.show()"
      ],
      "id": "51977b1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ao analisar a nota dada por cada produto mais vendido, parece que não há nenhuma preferencia dos consumidores. A tabela abaixo mostra essa a nota média por cada produto \n"
      ],
      "id": "9e292b43"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "(\n",
        "    pd.merge(\n",
        "        right=data_dict['Order_Items'],\n",
        "        left=data_dict['Products'],\n",
        "        how='left',\n",
        "        on='product_id')\n",
        "    .merge(\n",
        "        right=(join_order_items_and_products.product_category_name.value_counts(normalize=True) * 100).head(10).reset_index(),\n",
        "        left_on='product_category_name',\n",
        "        right_on='index',\n",
        "    ).merge(\n",
        "        right=data_dict['Order_Reviews'],\n",
        "        on='order_id'\n",
        "    )\n",
        "    [[\n",
        "        'product_category_name_x',\n",
        "        'review_score'\n",
        "    ]]\n",
        "    .groupby(['product_category_name_x'])\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .assign(review_score = lambda x: round(x.review_score, 2),\n",
        "            product_category_name_x = lambda x: x.product_category_name_x.str.replace(\"_\", \" \"))\n",
        "    .rename(columns={\n",
        "        \"product_category_name_x\" : \"Produto\",\n",
        "        \"review_score\" : \"Avaliação\"\n",
        "    })\n",
        ")"
      ],
      "id": "39cc1513",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar na tabela abaixo, há uma quantidade considerável de outliers no conjunto de dados em relação ao valor total pago. Uma pequena quantidade de consumidores gastaram alguns milhares de reais, enquanto a vasta maioria tem seu ticket médio por volta de 90 reais.\n"
      ],
      "id": "31b27a36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "join_order_items_and_products.total_cost.describe().to_frame().T[['mean', 'std', 'min', '25%', '50%', '75%', 'max']].rename(columns={\"50%\" : \"Mediana\", \"mean\" : 'Media', 'std' : 'Desvio Padrão', 'max' : 'Max', 'min' : 'Min'})"
      ],
      "id": "e2bb468c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tipo de Pagamento\n",
        "\n",
        "O tipo de pagamento mais utilizado em compras é, com uma grande diferença, o cartão de crédito, provavelmente devido a sua habilidade de parcelar suas compras.\n"
      ],
      "id": "9f8f01bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "p = (\n",
        "    so.Plot(\n",
        "        data = (data_dict['Order_Payments'].payment_type.value_counts(normalize=True) * 100).reset_index(),\n",
        "        x = 'index',\n",
        "        y = 'payment_type'\n",
        "    )\n",
        "    .add(so.Bar())\n",
        "    .layout(size=(15, 8))\n",
        ")\n",
        "\n",
        "p.show()\n"
      ],
      "id": "c00abe50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apesar disso, cerca de 50% das vendas foram a vista e das que foram parceladas houve uma clara tendencia a um menor parcelamento possível. Esse fenomeno talvez seja devido ao baixo ticket médio do e-commerce, já que valores mais altos tendem a serem pagos em maiores parcelas. As tabelas abaixo mostram esses fenomenos.\n"
      ],
      "id": "884315ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "(data_dict['Order_Payments'].payment_installments.value_counts(normalize=True) * 100).reset_index().rename(columns={'index' : 'payment_installments', 'payment_installments' : 'Perc_total'}).assign(Perc_total = lambda x: round(x.Perc_total, 2)).head(10)"
      ],
      "id": "1c17dbf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelagem\n",
        "\n",
        "Meu objetivo é criar um modelo que crie cluster de clientes com base em suas características. Para tal, utilizarei o algoritmo K-Means para o agrupamento e o método Elbow para determinar o número de clusters. Para isso, eu utilizarei o conceito de RFM (Recency, Frequency, Monetary) para criar as features do modelo. \n",
        "\n",
        "RFM é um modelo de análise de consumidores que utiliza três variáveis para classificar os clientes de acordo com seu comportamento de compra. A variável Recency (R) mede a o quão recente foi a compra dos clientes, a variável Frequency (F) mede a frequência de compra dos clientes e a variável Monetary (M) mede o valor total gasto pelos clientes. Com essas informações é possível melhorar a tomada de decisão de uma empresa. A tabela abaixo desse [site](https://www.putler.com/rfm-analysis/) nos mostra como sabendo o perfil do consumidor através dessa metodologia pode ajudar na tomada de decisão empresarial\n",
        "\n",
        "|Customer Segment | \tActivity | \tActionable Tip|\n",
        "|--------------|-----------|------------|\n",
        "|Champions | \tMade recent purchases, frequent purchases, high spending | \tOffer loyalty rewards or exclusive promotions to maintain their loyalty and encourage them to make repeat purchases.|\n",
        "|Loyal Customers | \tMade frequent purchases, but not recently, high spending | \tSend them personalized offers or promotions based on their past purchases to encourage them to return and make another purchase.|\n",
        "|Potential Loyalists | \tMade recent purchases, but not frequent, high spending | \tProvide personalized recommendations for related products or services that they may be interested in based on their recent purchase.|\n",
        "|Recent Customers | \tMade recent purchases, but not frequent, low spending | \tProvide a discount or offer on their next purchase to encourage them to make another purchase and become a potential loyal customer.|\n",
        "|Promising Customers | \tMade frequent purchases, but not recently, low spending | \tProvide them with targeted offers or discounts on products or services they have shown interest in, to encourage them to return and make another purchase.|\n",
        "|At-Risk Customers | \tMade frequent purchases in the past, but not recently, low spending | \tSend them personalized offers or discounts to encourage them to make another purchase and re-engage with your brand.|\n",
        "|Lost Customers | \tNo recent purchases, low frequency, low spending | \tRe-engage them by sending personalized emails or promotions to encourage them to return and make another purchase.|\n",
        "|Lost Cheap Customers | \tNo recent purchases, low frequency, low spending | \tOffer them a discount or promotion to encourage them to return and make another purchase, but also consider whether it makes sense to focus on acquiring new customers instead.|\n",
        "\n",
        "Geralmente, as variáveis da análise por RFM são posta em uma escala de 1 a 5. Para esse trabalho, foi testado ambos os casos usando o algoritmo K-means. Uma forma de metrificar os clusters é calculando sua distorção que, dentro do contexto do K-means, calcula a diferença entre o centro dos clusters e cada ponto dos dados. Dessa forma, cada vez menor a distorção, mais representativo é o cluster. Evidentemente, a simples minimização da distorção sem limitações na quantidade de clusters nos levaria a uma situação que teriamos a mesma quantidade de observações que de clusters. Nesse contexto, utilizamos o \"elbow method\" para balancear o trade-off entre minimizar a distorção e minimizar a quantidade de clusters. Essa técnica consiste em calcular a variação da distorção a cada aumento marginal na quantidade de clusters. Quando a variação da distorção for muito pequena, isto é, quando a curva distorção vs quantidade de clusters ser quase plana que parece um cotovelo, seria a quantidade ideal de clusters.  \n",
        "\n",
        "Nos gráficos abaixo foi realizado essa técnica tanto quando os dados de RFM não está em escala de 1 a 5 e quando este está, respectivamente. A conclusão é de que o número ideal de clusters é 4 e que o conjunto de dados ideal para a realização da clusterização é o que teve sua escala de 1 a 5, de acordo com o valor de distorção. Os gráficos abaixo mostram essa perspectiva.\n"
      ],
      "id": "5767a387"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from plotnine import *\n",
        "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = (\n",
        "    pd.merge(\n",
        "        left=data_dict['Customers'],\n",
        "        right=data_dict['Orders'],\n",
        "        on='customer_id',\n",
        "        how='inner'\n",
        "    ).merge(\n",
        "        right=data_dict['Order_Payments'],\n",
        "        on='order_id',\n",
        "        how='inner'\n",
        "    )\n",
        "    [[\n",
        "        'customer_unique_id',\n",
        "        'order_purchase_timestamp',\n",
        "        'payment_value'\n",
        "    ]]\n",
        ")\n",
        "\n",
        "\n",
        "def create_rfm_dataframe(data: pd.DataFrame):\n",
        "    #  Nos metadados do dataset, a data final é 31/12/2018\n",
        "    last_dataset_day = pd.to_datetime('31/12/2018', format='%d/%m/%Y')\n",
        "    \n",
        "    df_monetary = (\n",
        "                    data\n",
        "                    [['customer_unique_id', 'payment_value']]\n",
        "                    .groupby('customer_unique_id')\n",
        "                    .sum()\n",
        "                    .rename(columns={'payment_value': 'Monetary'})\n",
        "                    .reset_index()\n",
        "                )\n",
        "    df_recency = (\n",
        "                    data\n",
        "                    [['customer_unique_id', 'order_purchase_timestamp']]\n",
        "                    .groupby('customer_unique_id')\n",
        "                    .max()\n",
        "                    .assign(order_purchase_timestamp=lambda x: (last_dataset_day - x.order_purchase_timestamp).dt.days)\n",
        "                    .rename(columns={'order_purchase_timestamp': 'Recency'})\n",
        "                    .reset_index()\n",
        "                )\n",
        "    df_frequency = (\n",
        "                    data\n",
        "                    [['customer_unique_id', 'order_purchase_timestamp']]\n",
        "                    .groupby('customer_unique_id')\n",
        "                    .count()\n",
        "                    .rename(columns={'order_purchase_timestamp': 'Frequency'})\n",
        "                    .reset_index()\n",
        "                )\n",
        "    \n",
        "    return pd.merge(\n",
        "        left=df_monetary,\n",
        "        right=df_recency,\n",
        "        on='customer_unique_id',\n",
        "    ).merge(\n",
        "        right=df_frequency,\n",
        "        on='customer_unique_id',\n",
        "    )\n",
        "    \n",
        "    \n",
        "rfm_df = create_rfm_dataframe(data)\n",
        "\n",
        "\n",
        "rfm_df = rfm_df.query('not(Monetary == 0)')\n",
        "\n",
        "\n",
        "def frequency_count(x):\n",
        "    if x <= 1:\n",
        "        return 1\n",
        "    elif x <= 3:\n",
        "        return 2\n",
        "    elif x <= 5:\n",
        "        return 3\n",
        "    elif x <= 8:\n",
        "        return 4\n",
        "    else:\n",
        "        return 5\n",
        "    \n",
        "\n",
        "rfm_df = rfm_df.assign(\n",
        "    Monetary_test = pd.qcut(rfm_df['Monetary'], 5, labels=[1, 2, 3, 4, 5]),\n",
        "    Recency_test = pd.qcut(rfm_df['Recency'], 5, labels=[5, 4, 3, 2, 1]),\n",
        "    Frequency_test = rfm_df['Frequency'].apply(frequency_count)\n",
        ")\n",
        "\n",
        "\n",
        "a = kelbow_visualizer(KMeans(), X= rfm_df[['Monetary', 'Recency', 'Frequency']], k=(1,20), show=False)"
      ],
      "id": "9b8369e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "\n",
        "a = kelbow_visualizer(KMeans(), X=rfm_df[['Monetary_test', 'Recency_test', 'Frequency_test']].apply(lambda x: pd.to_numeric(x)), k=(1,20), show=False)"
      ],
      "id": "d863f02f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Após verificado quais melhores métricas e conjuntos de dados para realizar o agrupamento dos clientes, podemos classificar os clientes em 4 grupos:\n",
        "\n",
        "- Grupo 1: Clientes que gastaram muito dinheiro, mas que não compraram com frequência e que não compraram recentemente.\n",
        "- Grupo 2: Clientes que compraram pouco, não compraram recentemente e não compraram com frequência.\n",
        "- Grupo 3: Clientes que compraram recentemente, mas que não compraram com frequência e que gastaram uma quantia média de dinheiro.\n",
        "- Grupo 4: Clientes que compraram com frequência, compraram uma quantidade razoável de dinheiro e que compraram relativamente recentemente.\n"
      ],
      "id": "25a535f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from plotnine import *\n",
        "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
        "\n",
        "data_dict = import_data()\n",
        "\n",
        "data = (\n",
        "    pd.merge(\n",
        "        left=data_dict['Customers'],\n",
        "        right=data_dict['Orders'],\n",
        "        on='customer_id',\n",
        "        how='inner'\n",
        "    ).merge(\n",
        "        right=data_dict['Order_Payments'],\n",
        "        on='order_id',\n",
        "        how='inner'\n",
        "    )\n",
        "    [[\n",
        "        'customer_unique_id',\n",
        "        'order_purchase_timestamp',\n",
        "        'payment_value'\n",
        "    ]]\n",
        ")\n",
        "\n",
        "def create_rfm_dataframe(data: pd.DataFrame):\n",
        "    #  Nos metadados do dataset, a data final é 31/12/2018\n",
        "    last_dataset_day = pd.to_datetime('31/12/2018', format='%d/%m/%Y')\n",
        "    \n",
        "    df_monetary = (\n",
        "                    data\n",
        "                    [['customer_unique_id', 'payment_value']]\n",
        "                    .groupby('customer_unique_id')\n",
        "                    .sum()\n",
        "                    .rename(columns={'payment_value': 'Monetary'})\n",
        "                    .reset_index()\n",
        "                )\n",
        "    df_recency = (\n",
        "                    data\n",
        "                    [['customer_unique_id', 'order_purchase_timestamp']]\n",
        "                    .groupby('customer_unique_id')\n",
        "                    .max()\n",
        "                    .assign(order_purchase_timestamp=lambda x: (last_dataset_day - x.order_purchase_timestamp).dt.days)\n",
        "                    .rename(columns={'order_purchase_timestamp': 'Recency'})\n",
        "                    .reset_index()\n",
        "                )\n",
        "    df_frequency = (\n",
        "                    data\n",
        "                    [['customer_unique_id', 'order_purchase_timestamp']]\n",
        "                    .groupby('customer_unique_id')\n",
        "                    .count()\n",
        "                    .rename(columns={'order_purchase_timestamp': 'Frequency'})\n",
        "                    .reset_index()\n",
        "                )\n",
        "    \n",
        "    return pd.merge(\n",
        "        left=df_monetary,\n",
        "        right=df_recency,\n",
        "        on='customer_unique_id',\n",
        "    ).merge(\n",
        "        right=df_frequency,\n",
        "        on='customer_unique_id',\n",
        "    )\n",
        "    \n",
        "    \n",
        "rfm_df = create_rfm_dataframe(data)\n",
        "\n",
        "\n",
        "\n",
        "rfm_df = rfm_df.query('not(Monetary == 0)')\n",
        "\n",
        "def frequency_count(x):\n",
        "    if x <= 1:\n",
        "        return 1\n",
        "    elif x <= 3:\n",
        "        return 2\n",
        "    elif x <= 5:\n",
        "        return 3\n",
        "    elif x <= 8:\n",
        "        return 4\n",
        "    else:\n",
        "        return 5\n",
        "rfm_df = rfm_df.assign(\n",
        "    Monetary_test = pd.qcut(rfm_df['Monetary'], 5, labels=[1, 2, 3, 4, 5]),\n",
        "    Recency_test = pd.qcut(rfm_df['Recency'], 5, labels=[5, 4, 3, 2, 1]),\n",
        "    Frequency_test = rfm_df['Frequency'].apply(frequency_count)\n",
        ")\n",
        "\n",
        "def train_kmeans(_df: pd.DataFrame, n_clusters: int, have_df_scaled: bool = False) -> tuple[KMeans, pd.DataFrame]:\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(_df)\n",
        "    model = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto').fit(df_scaled)\n",
        "    if have_df_scaled:\n",
        "        column_name = scaler.get_feature_names_out()\n",
        "        _df = pd.DataFrame(df_scaled).assign(cluster=model.labels_).rename(columns={\n",
        "            0: column_name[0],\n",
        "            1: column_name[1],\n",
        "            2: column_name[2]\n",
        "        })\n",
        "    else:\n",
        "        _df = pd.DataFrame(_df).assign(cluster=model.labels_)\n",
        "    \n",
        "    \n",
        "    return model, _df\n",
        "def cluster_data_transformation(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    return pd.melt(\n",
        "        frame = (data\n",
        "                .drop(columns=['Consumer_unique_id'])\n",
        "                .apply(lambda x: pd.to_numeric(x))\n",
        "                .groupby('cluster')\n",
        "                .agg({\n",
        "                    'Monetary_test' : 'mean',\n",
        "                    'Recency_test' : 'mean',\n",
        "                    'Frequency_test' : 'mean'\n",
        "                })\n",
        "                .reset_index()\n",
        "                .assign(Consumer_unique_id = data['Consumer_unique_id'])),\n",
        "        id_vars = ['cluster', 'Consumer_unique_id'],\n",
        "        var_name=\"type\", \n",
        "        value_name=\"value\"\n",
        "    )\n",
        "\n",
        "modelo, dados_cluster = train_kmeans(rfm_df[['Monetary_test', 'Recency_test', 'Frequency_test']], 4, have_df_scaled = False)\n",
        "\n",
        "dados_cluster = dados_cluster.assign(Consumer_unique_id = rfm_df['customer_unique_id'], cluster = lambda x: x.cluster + 1)\n",
        "\n",
        "normal_cluster_data = cluster_data_transformation(dados_cluster)\n",
        "\n",
        "fig, plot = (\n",
        "    ggplot(data=normal_cluster_data) +\n",
        "    aes(x='cluster', y='value', fill='type') + \n",
        "    geom_bar(stat='identity', position='dodge', alpha=0.8, color='black', size=0.2) +\n",
        "    labs(x='Clusters', y='Valores', title='Valor por Cluster', fill='Tipo') +\n",
        "    guides(fill=guide_legend(title='')) + \n",
        "    scale_fill_manual(values=['#1f77b4', '#ff7f0e', '#2ca02c']) +\n",
        "    theme_bw() +\n",
        "    theme(plot_title=element_text(size=14, face='bold', margin={'b': 10}),\n",
        "       axis_title=element_text(size=12),\n",
        "       legend_position='bottom',\n",
        "       legend_title=element_text(size=10, face='bold'),\n",
        "       panel_grid_major=element_blank(),\n",
        "       panel_grid_minor=element_blank(),\n",
        "       panel_border=element_blank(),\n",
        "       axis_line=element_line(color='black'),\n",
        "       axis_ticks=element_line(color='black'),\n",
        "       legend_background=element_blank(),\n",
        "       legend_key=element_blank(),\n",
        "       figure_size=(8, 6)) +\n",
        "    ylim(0, 5)\n",
        ").draw(return_ggplot=True, show=True)\n",
        "\n",
        "fig.show()\n"
      ],
      "id": "db6ea949",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Atualmente na empresa, como podemos observar no gráfico abaixo, o cluster mais significativo é o 3 e o menos significativo é o 4. Dessa forma, podemos concluir que a Olist conseguiu chamar a atenção de novos clientes que gastam uma boa quantidade de dinheiro, já que a única diferença entre o 3 e o 4 grupo é a frequencia. Dado esse contexto, em vista o paradigma RFM, é evidente que é o ideal para a marca é criar produtos que aumente fidelização de seus clientes como pacotes de assinatura e melhorar sistema de recomendação de produtos. \n"
      ],
      "id": "d8209672"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "fig, plot = (\n",
        "    ggplot(data=(dados_cluster\n",
        "                .groupby('cluster')\n",
        "                .agg({'Consumer_unique_id' : 'count'})\n",
        "                .rename(columns={'Consumer_unique_id' : 'count'})\n",
        "                .reset_index()\n",
        "                .apply(lambda x: x if x.name == 'cluster' else (x / x.sum()) * 100)\n",
        "                .sort_values(by='count', ascending=False))) +\n",
        "    aes(x='cluster', y='count', fill='cluster') + \n",
        "    geom_bar(stat='identity', position='dodge', alpha=0.8, color='black', size=0.2) +\n",
        "    labs(x='Clusters', y='Quantidade (%)', title='Quantidade de Consumidor por Cluster') +\n",
        "    theme_classic() +\n",
        "    theme(plot_title=element_text(size=14, face='bold', margin={'b': 10}),\n",
        "       axis_title=element_text(size=12),\n",
        "       legend_position='bottom',\n",
        "       legend_title=element_text(size=10, face='bold'),\n",
        "       panel_grid_major=element_blank(),\n",
        "       panel_grid_minor=element_blank(),\n",
        "       panel_border=element_blank(),\n",
        "       axis_line=element_line(color='black'),\n",
        "       axis_ticks=element_line(color='black'),\n",
        "       legend_background=element_blank(),\n",
        "       legend_key=element_blank(),\n",
        "       figure_size=(8, 6)) +\n",
        " guides(fill=guide_legend(ncol=3, title=''))\n",
        "\n",
        ").draw(return_ggplot=True, show=True)\n"
      ],
      "id": "aa79d0b9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}